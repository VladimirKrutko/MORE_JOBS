{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# from scripting.parser.pracuj_pl.pracuj_pl import PracujPLParser\n",
    "import pdb\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from scripting.parser.base_parser import BaseParser\n",
    "\n",
    "class PracujPLParser(BaseParser):\n",
    "    JSON_PATTERN = r\"window\\['kansas-offerview'\\]\\s*=\\s*(\\{.*?\\});\"\n",
    "    RESULT_TEMPLATE ={\n",
    "            'url': None,\n",
    "            'site': 'pracuj.pl',\n",
    "            'company_name': None,\n",
    "            'company_url': None,\n",
    "            'company_description': None,\n",
    "            'offer_title': None,\n",
    "            'position_level': None,\n",
    "            'technology_list': None,\n",
    "            'offer_description': None,\n",
    "            'requirements': None,\n",
    "            'responsibilities': None,\n",
    "            'language': None,\n",
    "            'city': None,\n",
    "            'salary': None,\n",
    "            'work_type': None,\n",
    "            }\n",
    "    WORKING_TIME = 8\n",
    "    WORKING_DAYS = 20\n",
    "    \n",
    "    def __init__(self, parsed_site):\n",
    "        super().__init__(parsed_site)\n",
    "        \n",
    "    JSON_PATHS = {\n",
    "        'company_name': ['props', 'pageProps', 'dehydratedState', 'queries', 0, 'state', 'data', 'attributes', 'displayEmployerName'],\n",
    "        'company_description': ['props', 'pageProps', 'dehydratedState', 'queries', 0, 'state', 'data', 'sections'],\n",
    "        'offer_title': ['props', 'pageProps', 'dehydratedState', 'queries', 0, 'state', 'data', 'attributes', 'jobTitle'],\n",
    "        'position_level': ['props', 'pageProps', 'dehydratedState', 'queries', 0, 'state', 'data', 'attributes', 'employment', 'positionLevels'],\n",
    "        'technologies': ['props', 'pageProps', 'dehydratedState', 'queries', 0, 'state', 'data', 'textSections'],\n",
    "        'requairemtns': ['props', 'pageProps', 'dehydratedState', 'queries', 0, 'state', 'data', 'sections'],\n",
    "        'responsibilities': ['props', 'pageProps', 'dehydratedState', 'queries', 0, 'state', 'data', 'sections'],\n",
    "        'language': ['props', 'pageProps', 'dehydratedState', 'queries', 0, 'state', 'data', 'jobOfferLanguage', 'isoCode'],\n",
    "        'salary': ['props', 'pageProps', 'dehydratedState', 'queries', 0, 'state', 'data', 'attributes', 'employment', 'typesOfContracts'],\n",
    "        'work_type': ['props', 'pageProps', 'dehydratedState', 'queries', 0, 'state', 'data', 'attributes', 'employment', 'workModes'],\n",
    "        'locaiton': ['props', 'pageProps', 'dehydratedState', 'queries', 0, 'state', 'data', 'attributes', 'workplaces'],\n",
    "    }\n",
    "    \n",
    "    def initalize_variables(self, page_content, url):\n",
    "        self.url = url\n",
    "        self.doc = BeautifulSoup(page_content, 'html.parser')\n",
    "        self.page_json = self.extract_json()\n",
    "        \n",
    "    def parse(self, page_content, url):\n",
    "        result = self.RESULT_TEMPLATE.copy()\n",
    "        self.initalize_variables(page_content, url)\n",
    "        self.parse_offer_data(result)\n",
    "        return result\n",
    "    \n",
    "    def parse_offer_data(self, result):\n",
    "        result['url'] = self.url\n",
    "        result['company_name'] = self.get_json_value(self.page_json, self.JSON_PATHS['company_name'])\n",
    "        result['company_description'] = self.parse_company_description()\n",
    "        result['offer_title'] = self.get_json_value(self.page_json, self.JSON_PATHS['offer_title'])\n",
    "        result['position_level'] = self.get_json_value(self.page_json, self.JSON_PATHS['position_level'])\n",
    "        result['technology_list'] = self.parse_technology_list()\n",
    "        result['requirements'] = self.parse_rr_section('requirements')\n",
    "        result['responsibilities'] = self.parse_rr_section('responsibilities')\n",
    "        result['language'] = self.get_json_value(self.page_json, self.JSON_PATHS['language'])\n",
    "        result['salary'] = self.parse_salary()\n",
    "        result['work_type'] = self.parse_work_type()\n",
    "        # self.get_json_value(self.page_json, self.JSON_PATHS['work_type'])\n",
    "        result['city'] = self.parse_location('city')\n",
    "        result['country'] = self.parse_location('country')\n",
    "    \n",
    "    \n",
    "    def parse_work_type(self):\n",
    "        work_type = self.get_json_value(self.page_json, self.JSON_PATHS['work_type'])\n",
    "        return [wt['code'] for wt in work_type] if work_type else None\n",
    "    \n",
    "    def parse_location(self, location_type):\n",
    "        location_data = self.get_json_value(self.page_json, self.JSON_PATHS['locaiton'])\n",
    "        location_result = set()\n",
    "        for location in location_data:\n",
    "            match location_type:\n",
    "                case 'city':\n",
    "                    location_result.add(location['inlandLocation']['location']['name'])\n",
    "                case 'country':\n",
    "                    location_result.add(location['country']['name'])\n",
    "                case _:\n",
    "                    location_result.add(None)\n",
    "        return location_result                            \n",
    "\n",
    "    def parse_work_type(self):\n",
    "        work_type = self.get_json_value(self.page_json, self.JSON_PATHS['work_type'])\n",
    "        return [wt['code'] for wt in work_type] if work_type else None\n",
    "    \n",
    "    def parse_salary(self):\n",
    "        salary_data = self.get_json_value(self.page_json, self.JSON_PATHS['salary'])\n",
    "        process_salary = lambda salary: {\n",
    "            'salary_amount': self.salary_amount(salary['salary']), \n",
    "            'employment_type': salary['name'], \n",
    "            'currency': salary['salary']['currency']['code']\n",
    "            }\n",
    "        return list(map(process_salary, salary_data)) if salary_data else None\n",
    "    \n",
    "    def salary_amount(self, salary):\n",
    "        # pdb.set_trace()\n",
    "        salary_amount = (\n",
    "            f\"{salary['from'] * self.WORKING_TIME * self.WORKING_DAYS}-{salary['to'] * self.WORKING_TIME * self.WORKING_DAYS}\"\n",
    "            if salary['timeUnit']['longForm']['name'] != 'monthly'\n",
    "            else f\"{salary['from']}-{salary['to']}\"\n",
    "        )\n",
    "        return salary_amount        \n",
    "\n",
    "    def parse_rr_section(self, section_name):\n",
    "        base_section = self.get_json_value(self.page_json, self.JSON_PATHS['company_description'])\n",
    "        desctiption_section = self.find_section(base_section, section_name)\n",
    "        join_bullets = lambda section: \"@@@@\".join(section['model']['bullets'])\n",
    "        if desctiption_section.get('subSections'):\n",
    "            return \"@@@@\".join([join_bullets(sub_section) for sub_section in desctiption_section['subSections']]) if desctiption_section else None\n",
    "        else:\n",
    "            return join_bullets(desctiption_section) if desctiption_section else None\n",
    "    \n",
    "    def parse_technology_list(self):\n",
    "        technology_list = {}\n",
    "        offer_technologies = self.get_json_value(self.page_json, self.JSON_PATHS['technologies'])\n",
    "        get_techology = lambda tesh: next((tech for tech in offer_technologies if tech['sectionType'] == tesh), {})\n",
    "        technology_list['required'] = get_techology('technologies-expected').get('textElements')\n",
    "        technology_list['expected'] = get_techology('technologies-optional').get('textElements')\n",
    "        return technology_list\n",
    "        \n",
    "    def parse_company_description(self):\n",
    "        base_section = self.get_json_value(self.page_json, self.JSON_PATHS['company_description'])\n",
    "        desctiption_section = self.find_section(base_section, 'about-us-description')\n",
    "        return self.squish(\" \".join(desctiption_section['model']['paragraphs'])) if desctiption_section else None\n",
    "        \n",
    "    def find_section(self, seection_list, section_name):\n",
    "        return next((section for section in seection_list if section['sectionType'] == section_name), None)\n",
    "        \n",
    "    def squish(text):\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "    def extract_json(self):\n",
    "        json_text = self.doc.find('script', id='__NEXT_DATA__').string\n",
    "        return json.loads(json_text.replace(\"'\", '\"').replace('undefined', '\"undefined\"'))\n",
    "    \n",
    "    def get_json_value(self, json_obj, json_path, key_index=0):\n",
    "        try:\n",
    "            return self.get_json_value(json_obj[json_path[key_index]], json_path, key_index + 1)\n",
    "        except:\n",
    "            return json_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://www.pracuj.pl/praca/ml-ops-engineer-warszawa,oferta,1003733553',\n",
       " 'site': 'pracuj.pl',\n",
       " 'company_name': 'DCG',\n",
       " 'company_url': None,\n",
       " 'company_description': None,\n",
       " 'offer_title': 'ML Ops Engineer',\n",
       " 'position_level': [{'id': 4,\n",
       "   'name': 'specialist (Mid / Regular)',\n",
       "   'pracujPlName': 'specjalista (Mid / Regular)'}],\n",
       " 'technology_list': {'required': ['Databricks', 'Azure Cosmos DB', 'ML Ops'],\n",
       "  'expected': ['Dash', 'Shiny R', 'Streamlit']},\n",
       " 'offer_description': None,\n",
       " 'requirements': 'At least 2 years of professional experience in ML Ops or ML engineering, with a focus on deploying and scaling machine learning models in production.@@@@Proficient in English, both written and spoken, suitable for business communication.@@@@Extensive knowledge of the Azure cloud environment and Databricks, including setup and maintenance as an ML platform.@@@@Demonstrated expertise in software development best practices, such as testing, continuous integration, and the use of DevOps tools.@@@@Advanced skills in Python and SQL, with experience in version control systems (e.g., Git) and building ML and data pipelines.@@@@Strong understanding of the data science lifecycle and the workflows data scientists use to create business value.@@@@Familiarity with agile development methodologies, such as SCRUM or Kanban.@@@@Commitment to writing clear, maintainable, and accurate code for efficient development and implementation.@@@@Knowledge of KNIME and related tools.@@@@Experience deploying ML tools with user interfaces, such as Dash, Shiny R, or Streamlit, in production.@@@@Experience mentoring or coaching junior team members in areas of expertise.@@@@Excellent communication skills for explaining complex ML Ops topics to audiences with varying levels of technical expertise.',\n",
       " 'responsibilities': 'Develop, optimize, and standardize scalable data science and machine learning solutions within cloud environments, particularly Azure.@@@@Contribute to the full lifecycle of data science projects by applying DevOps principles, managing code, experiments, and models, and leveraging CI/CD pipelines alongside industry best practices.@@@@Write clean, efficient, and testable code to ensure high-quality implementation.@@@@Collaborate closely with engineering teams to enhance data utilization and improve the deployment of models in production environments.@@@@Lead the design, monitoring, troubleshooting, debugging, and incident management processes for machine learning pipelines.@@@@Serve as a trusted advisor and advocate for ML Ops best practices, providing guidance on scaling, throughput optimization, infrastructure, and deployment strategies to the team and stakeholders.',\n",
       " 'language': 'en',\n",
       " 'city': {'Warszawa'},\n",
       " 'salary': [{'salary_amount': '90-125',\n",
       "   'employment_type': 'B2B contract',\n",
       "   'currency': 'PLN'}],\n",
       " 'work_type': [{'description': '',\n",
       "   'code': 'home-office',\n",
       "   'name': 'home office work',\n",
       "   'pracujPlName': 'praca zdalna'}],\n",
       " 'country': {'Poland'}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = PracujPLParser('pracuj.pl')\n",
    "with open('/Users/ulakruts/Personal Project/Praca_Inzynierska/MJ/test.html', 'r') as f:\n",
    "    html = f.read()\n",
    "    \n",
    "parser.parse(html, 'https://www.pracuj.pl/praca/ml-ops-engineer-warszawa,oferta,1003733553')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhtml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://www.pracuj.pl/praca/ml-ops-engineer-warszawa,oferta,1003733553\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Personal Project/Praca_Inzynierska/MJ/MORE_JOBS/scripting/parser/pracuj_pl/pracuj_pl.py:51\u001b[0m, in \u001b[0;36mPracujPLParser.parse\u001b[0;34m(self, page_content, url)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, page_content, url):\n\u001b[1;32m     50\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mRESULT_TEMPLATE\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitalize_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_offer_data(result)\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Personal Project/Praca_Inzynierska/MJ/MORE_JOBS/scripting/parser/pracuj_pl/pracuj_pl.py:47\u001b[0m, in \u001b[0;36mPracujPLParser.initalize_variables\u001b[0;34m(self, page_content, url)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl \u001b[38;5;241m=\u001b[39m url\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc \u001b[38;5;241m=\u001b[39m BeautifulSoup(page_content, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpage_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Personal Project/Praca_Inzynierska/MJ/MORE_JOBS/scripting/parser/pracuj_pl/pracuj_pl.py:134\u001b[0m, in \u001b[0;36mPracujPLParser.extract_json\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_json\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    133\u001b[0m     json_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscript\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__NEXT_DATA__\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstring\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mloads(json_text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mundefined\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mundefined\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'scripting.parser' from '/Users/ulakruts/Personal Project/Praca_Inzynierska/MJ/MORE_JOBS/scripting/parser/__init__.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scripting.parser\n",
    "\n",
    "\n",
    "scripting.parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('proxy_lust.json', 'r') as f:\n",
    "    proxy_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_proxy_list(input_file, output_file):\n",
    "    with open(input_file, 'r') as f:\n",
    "        proxy_data = json.load(f)['data']\n",
    "\n",
    "    transformed_list = []\n",
    "    for proxy in proxy_data:\n",
    "        transformed_list.append({\n",
    "            \"url\": f\"{proxy['protocols'][0]}://{proxy['ip']}:{proxy['port']}\",\n",
    "            \"banned\": False\n",
    "        })\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(transformed_list, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_proxy_list('proxy_lust.json', 'proxy_lust_transformed.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'playwright'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplaywright\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msync_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sync_playwright\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'playwright'"
     ]
    }
   ],
   "source": [
    "from playwright.sync_api import sync_playwright\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'offers': {'offer1': {'details': {'price': 100}}, 'offer2': {'details': {'price': 200}}}}\n"
     ]
    }
   ],
   "source": [
    "json_obj = {\n",
    "    \"offers\": {\n",
    "        \"offer1\": {\n",
    "            \"details\": {\n",
    "                \"price\": 100\n",
    "            }\n",
    "        },\n",
    "        \"offer2\": {\n",
    "            \"details\": {\n",
    "                \"price\": 200\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "json_path = [\"offers\", \"offer1\", \"details\", \"price\"]\n",
    "\n",
    "def get_offer_value(json_path, key_index=0, json_test=None):\n",
    "    porcess_json = json_test if json_test else json_obj\n",
    "    try:\n",
    "        return get_offer_value([json_path[key_index]], json_path, key_index + 1)\n",
    "    except:\n",
    "        return json_obj\n",
    "    # if json_obj.get(json_path[key_index]):\n",
    "    #     return get_offer_value(json_obj.get(json_path[key_index]), json_path, key_index + 1)\n",
    "    # else:\n",
    "    #     return json_obj[key_index]\n",
    "    \n",
    "print(get_offer_value(json_obj, json_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'base_login'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscripting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chrome_base\n",
      "File \u001b[0;32m~/Personal Project/Praca_Inzynierska/MJ/MORE_JOBS/scripting/login/Chrome_base.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplaywright\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msync_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sync_playwright\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbase_login\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseLogin\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPlaywrightLogin\u001b[39;00m(BaseLogin):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# Загрузка прокси из файла proxy_list.json\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'base_login'"
     ]
    }
   ],
   "source": [
    "from scripting.login import Chrome_base"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
