{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import CustomLoader\n",
    "import loader as Loader\n",
    "import parser as Parser\n",
    "from bs4 import BeautifulSoup\n",
    "from LoginScript import LoginScript\n",
    "from custom_parser.PageParser import PageParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# donload pages wiht job offers\n",
    "\n",
    "loader = Loader.Loader()\n",
    "base_url = 'https://it.pracuj.pl/praca'\n",
    "file_names = []\n",
    "for page_number in range(1, 98):\n",
    "    page_url = f\"{base_url}?pn={page_number}\" if page_number > 1 else base_url\n",
    "    file_names.append( loader.crawler(page_url, f'main_page_page{page_number}') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract job offers urls \n",
    "file_names = os.listdir('temp_data')\n",
    "for file_name in file_names:\n",
    "    base_file_name = file_name.split('_')[2]\n",
    "    parser = Parser.Parser( f\"./temp_data/{file_name}\", base_file_name)\n",
    "    parser.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dowload html offer pages\n",
    "todos = []\n",
    "for todo_file in os.listdir('todos'):\n",
    "    with open(f\"./todos/{todo_file}\") as f:\n",
    "        [todos.append(line.strip()) for line in f]\n",
    "\n",
    "cookies = False\n",
    "loader = Loader.Loader()\n",
    "for url2todo in todos:\n",
    "    if not(cookies):\n",
    "        cookies = LoginScript().click_button_and_get_cookies(url2todo)\n",
    "        cookies = { ck['name']:ck['value'] for ck in cookies}\n",
    "        \n",
    "    result = CustomLoader.CustomLoader.crawl(url2todo,'new_pages/offer', cookies)\n",
    "    if not(result):\n",
    "        cookies = LoginScript().click_button_and_get_cookies(url2todo)\n",
    "        cookies = { ck['name']:ck['value'] for ck in cookies}\n",
    "\n",
    "        with open('unparsable_todo.txt', 'a') as f:\n",
    "            f.write(url2todo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('./offer_data/new_pages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract necessary data from offer pages\n",
    "file_name = os.listdir('./offer_data/new_pages')\n",
    "for i in range(0, len(os.listdir('./offer_data/new_pages')), 500):\n",
    "    page_jsons = []\n",
    "    last = i+500 if i+500 < len(file_name) else len(file_name)\n",
    "    for file in file_name[i:last]:\n",
    "        page_jsons.append(PageParser(f\"./offer_data/new_pages/{file}\").parse_json())\n",
    "    with open(f'./offer_data/pl_json_pages/{int( time.time() *1000000 )}.json', 'w', encoding=\"utf-8\") as f:\n",
    "        json.dump(page_jsons, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.632"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir('./offer_data/new_pages'))/500"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mrenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
